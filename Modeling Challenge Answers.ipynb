{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial Plans: I am going to pursue the binary classification approach. I believe that this technique versus the multi-class classification will allow for increased statistical power (i.e., more training data) to isolate those nearing default from the 62k 'Current' population."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Future Plans: The binary classification model appears to work well (AUC=0.93). In a future version of this work, I do believe a multi-class classification model would be better. As mentioned in the challenge pdf, not all loans are created equally. For example, defaulting on a small loan is not nearly as problematic as someone defaulting on a large loan assuming equal default rates. To this end, I believe creating several tiers (classes) based on the loan amount will be important. Another possibility is to develop a model not only on the binary default outcome, but on the amount of the loan making this more of a regression-type problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Initial Plans: As mentioned in the modeling text, several of the variables required manipulation before they are used in the final binary classification model. Four of the features contained missing information (NA): emp_length, mths_since_last_delinq, mths_since_last_record, inq_last_12m. The first of these is a categorical variable, making it slightly more difficult to cleanse. I chose to create an indicator variable for the missing employment data and then set all the missing values equal to the resulting employment history mean of 6.07 years. Employment values below 1 year and greater than 10 years were set to 1 and 10 years, respectively. I did not explore alternative missing data techniques due to time constraints. The months since last delinquent payment seemed like a very important feature. In the event of missing data in this feature (presumably because there were no delinquent payments), I set it equal to the time since the loan was underwritten. The last two features (mths_since_last_record and inq_last_12m) I chose to exclude in the interest of time. In a future model, I would likely combine the two inq_last_* variables to avoid missing data and do something similar with the last record data.\n",
    "\n",
    "The other significant data cleansing task is to numerically-encode the categorical features. For most of the features I chose the standard label + one-hot encoding technique, which creates a binary feature for each unique categorical value. One of the features, purpose, had a number of unique values and experience tells me that this leads to poor results due to limited, and perhaps even null, non-zero values in the training/testing samples. I chose, by eye, to group together all values with less than 1,000 entries. In a future model, this could clearly be developed further (i.e., keeping more unique values seperate). \n",
    "\n",
    "The last task to complete was manipulating the four dates in the model: the loan creation date, the earliest line of credit date, last credit pull date, and lastly, the latest status date of 1/23/2017. For this, I chose to compute the relative time, in days, between these dates. For example, I created a new feature that is the time between the latest status date and the earliest line of credit date. For some IDs, this number was in the 10,000s indicating an experienced borrower. As mentioned in the text, I suspect that some form of data leakage occurs in this process as there were several negative times (e.g., issue dates more recent than credit pull dates). I ran two models w/ and w/o the credit pull date variable and observed between agreement between a logistic regression model and a boosted tree model, suggesting that this variable indeed somehow 'knows' about the output variable. A future model version would certainly investigate this future to see if any of this information is recoverable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "* By far, the most important feature in the model is the most recent FICO score. After this feature, many of the features provide incremental improvements to the model performance. For example, delinquent amount, missing employment information, and renting vs owning also contribute. The 'verification_status' feature seemed to provide very little lift. The 'purpose' feature also did not show as much lift as might be expected (i.e., it did not seem to matter what the loan was originally intended for). Also, it seems that the FICO score at the of time of loan is not strongly correlated with default, which seems odd and likely unfortunate."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "* One can always improve model performance by additional feature engineering and advanced statistical techniques. Many of the choices made in this assignment were made in the interest of time and simplicity. With additional time, I would focus more effort to understand the relationships between the features that did not improve performance. The precision of a statistical model will only ever be a fine as that of the data and therefore understanding the reason for NA values will be important. I also believe there is more to do with the time variables, especially the one excluded due to possible leakage."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "* Certainly a very low FICO score at the time of the loan is a red flag. Also, high numbers of delinquent fields (e.g., number of deliquent accounts or the value of those accounts) seems highly correlated with a future default. Any of these fields could be combined into a rules-based or business-based-decision without the assistance of an advanced statistical model. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "* Initially, I ran three models: a random forest (RF) model, a logistical regression model, and xgboost (XGB). Experience tells me that the RF and XGB models will outperform the logistical model, but a large relative difference between the performance is usually an indicator of overfitting. I observed this when including the time variables and the AUC of the two tree models seems too good to be true (>0.96). After removing the time variable, all three models performed equally well with an AUC of 0.93, which is still quite good. The final choice of model, in my experience, is usually a combination of the metric (e.g., AUC) as well as ones ability to explain the method to stakeholders."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "* The model difficulties rest primarily on the feature engineering, and not the machine learning process. One potential difficulty is scoring data in real-time and having a fast algorithm is important. That said, it's been my experience that all three models are trained quickly as well as score incoming data quickly."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "* In the interest of time, I kept all non-singular features in my model as experience tells me that tree-based models are quite adept at ignoring irrelevant features. A more formal model would likely limit the number of features based on some relative improvement metric such as expained variance. Monitoring the data over time is also very important and if any of the features 'drift' over time, either a new model is required or those features much be dropped from the list."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "* I validated this model using a test/train split (2/3 train and 1/3 test). I did not use any cross-validation technique (e.g., k-folds) in this analysis in the interest of time, but certainly, that is something I would likely explore in a future model. The three models trained behaved nearly identically between the train and test samples, which strong suggested we are not over/underfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
